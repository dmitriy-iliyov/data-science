{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmitriy-iliyov/data-science/blob/main/neural-network/rnn/notebook/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8RbyjLGWeRv",
        "outputId": "385756ba-01d2-43d5-e4e0-ea6a537b4f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Test request successful\n",
            "Coin parsing successful\n",
            "Prices scaled shape: (2159, 1)\n",
            "Recurrent hidden layer №1: (24, 128)\n",
            "    Previous hidden layer №1: (24, 128)\n",
            "Recurrent hidden layer №2: (128, 128)\n",
            "    Previous hidden layer №2: (128, 128)\n",
            "Output layer: (128, 1)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def coin_parsing(currency, days=90, interval='hourly'):\n",
        "\n",
        "    test_url = 'https://api.coingecko.com/api/v3/coins/' + currency\n",
        "    test_response = requests.get(test_url)\n",
        "\n",
        "    if test_response.status_code == 200:\n",
        "\n",
        "        print('Test request successful')\n",
        "        url = test_url + '/market_chart'\n",
        "\n",
        "        params = {\n",
        "            'vs_currency': 'usd',\n",
        "            'days': days\n",
        "        }\n",
        "\n",
        "        if interval != 'hourly':\n",
        "            params['interval'] = interval\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        coin_year_prices = response.json()\n",
        "        dates = []\n",
        "        prices = []\n",
        "\n",
        "        for daily_price in coin_year_prices['prices']:\n",
        "            dates.append(datetime.datetime.fromtimestamp(daily_price[0] / 1000))\n",
        "            prices.append(daily_price[1])\n",
        "        coin_year_prices_df = pd.DataFrame({'date': dates, 'price': prices})\n",
        "\n",
        "        print('Coin parsing successful')\n",
        "        return coin_year_prices_df\n",
        "    else:\n",
        "        print('Test request failed, status code ' + str(test_response.status_code))\n",
        "        print(test_response.text)\n",
        "        return None\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "\n",
        "def prepare_data(currency):\n",
        "\n",
        "    df = coin_parsing(currency, 90)\n",
        "    prices = df['price']\n",
        "\n",
        "    prices_scaled = scaler.fit_transform(prices.values.reshape(-1, 1))\n",
        "\n",
        "    print(f\"Prices scaled shape: {prices_scaled.shape}\")\n",
        "\n",
        "    time_step = 24\n",
        "    features = 1\n",
        "    train_data = []\n",
        "    train_answ = []\n",
        "\n",
        "    for i in range(len(prices_scaled) - time_step):\n",
        "        train_data.append(prices_scaled[i:i + time_step])\n",
        "        train_answ.append(prices_scaled[i + time_step])\n",
        "\n",
        "\n",
        "    train_data = np.array(train_data, dtype=np.float32)\n",
        "    train_answ = np.array(train_answ, dtype=np.float32)\n",
        "\n",
        "    k = int(train_data.shape[0] * 0.8)\n",
        "    train_data, test_data = train_data[:k], train_data[k:]\n",
        "    train_answ, test_answ = train_answ[:k], train_answ[k:]\n",
        "\n",
        "    train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], features)\n",
        "    test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], features)\n",
        "\n",
        "    return train_data, train_answ, test_data, test_answ\n",
        "\n",
        "\n",
        "def one_plot_s(ax, x, y, y_max, label, color, title, x_label, y_label):\n",
        "    ax.plot(x, y, label=label, color=color)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.set_ylim(0, y_max + y_max / 10)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    mplcursors.cursor(ax).connect(\"add\", lambda sel: sel.annotation.set_text(f\"{sel.target[1]:.4f}\"))\n",
        "\n",
        "\n",
        "def two_plot_s(ax, x1, y1, label1, color1, x2, y2, label2, color2, title, x_label, y_label):\n",
        "    ax.plot(x1, y1, label=label1, color=color1, marker='o', markersize=2)\n",
        "    ax.plot(x2, y2, label=label2, color=color2, marker='o', markersize=2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    y = y1 + y2\n",
        "    min_y = min(y)\n",
        "    max_y = max(y)\n",
        "    ax.set_ylim(min_y - min_y / 10, max_y + max_y / 10)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    mplcursors.cursor(ax).connect(\"add\", lambda sel: sel.annotation.set_text(f\"{sel.target[1]:.4f}\"))\n",
        "\n",
        "\n",
        "def one_fit_statistic(fit_data):\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    gs = fig.add_gridspec(0, 2, width_ratios=[1, 1], height_ratios=[1, 1], wspace=0.2, hspace=0.2)\n",
        "    plt.subplots_adjust(left=0.05, right=0.98, bottom=0.1, top=0.95, wspace=0.2)\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    one_plot_s(ax1, range(fit_data['epochs']), fit_data['accuracy'], 1, 'Accuracy', 'blue', 'Model Accuracy',\n",
        "             'Epochs', 'Accuracy')\n",
        "    hidden_layer_count = fit_data.get('hidden_layer_count', 1)\n",
        "    test_1_info = (f'Network: {fit_data[\"network\"]}\\n'\n",
        "                   f'Hidden layer count: {hidden_layer_count}\\n'\n",
        "                   f'Hidden Neurons: {fit_data[\"hidden_neurons_count\"]}\\n'\n",
        "                   f'Execution Time: {fit_data[\"execution_time\"]:.2f}s\\n'\n",
        "                   f'Batch Size: {fit_data[\"batch_size\"]}')\n",
        "    ax1.text(0.975, 0.05, test_1_info, fontsize=11, verticalalignment='bottom', horizontalalignment='right',\n",
        "             transform=ax1.transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    one_plot_s(ax2, range(fit_data['epochs']), fit_data['mse'], max(fit_data['mse']), 'MSE', 'red', 'Model MSE',\n",
        "             'Epochs', 'MSE')\n",
        "    mse_info = f\"MSE: {fit_data['mse'][-1]:.4f}\"\n",
        "    ax2.text(0.975, 0.05, mse_info, fontsize=11, verticalalignment='bottom', horizontalalignment='right',\n",
        "             transform=ax2.transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_json(file_path, data):\n",
        "    for key in data.keys():\n",
        "        if isinstance(data[key], Iterable) and not isinstance(data[key], str):\n",
        "            data[key] = [float(i) for i in data[key]]\n",
        "    with open(file_path, 'a') as file:\n",
        "        file.write(json.dumps(data) + '\\n')\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "home_dir = '/content/drive/MyDrive/main/languages/Python/neural_network/labs/rnn/model'\n",
        "\n",
        "\n",
        "class RNN(tf.Module):\n",
        "\n",
        "    def __init__(self, input_neuron_count, hidden_neuron_count, output_neuron_count, hidden_layer_count):\n",
        "        super().__init__()\n",
        "\n",
        "        if hidden_layer_count != len(hidden_neuron_count):\n",
        "            raise ValueError(\"hidden_layer_count != len(hidden_neuron_count)\")\n",
        "\n",
        "        self._input_neuron_count = input_neuron_count\n",
        "        self._hidden_layer_count = hidden_layer_count\n",
        "        self._hidden_neurons_counts = hidden_neuron_count\n",
        "\n",
        "        self._hidden_w_list = []\n",
        "        self._hidden_b_list = []\n",
        "        self._context_hidden_w_list = []\n",
        "\n",
        "        previous_neuron_count = self._input_neuron_count\n",
        "\n",
        "        for i in range(len(self._hidden_neurons_counts)):\n",
        "            self._hidden_w_list.append(\n",
        "                tf.Variable(tf.random.uniform([previous_neuron_count, self._hidden_neurons_counts[i]], -1, 1), dtype=tf.float32))\n",
        "            self._hidden_b_list.append(\n",
        "                tf.Variable(tf.zeros([self._hidden_neurons_counts[i]]), dtype=tf.float32))\n",
        "            self._context_hidden_w_list.append(\n",
        "                tf.Variable(tf.random.uniform([previous_neuron_count, self._hidden_neurons_counts[i]], -1, 1), dtype=tf.float32))\n",
        "            previous_neuron_count = self._hidden_neurons_counts[i]\n",
        "\n",
        "        self._output_neuron_count = output_neuron_count\n",
        "        self._output_w = tf.Variable(tf.random.uniform([self._hidden_neurons_counts[-1], self._output_neuron_count], -1, 1),\n",
        "                                     dtype=tf.float32)\n",
        "        self._output_b = tf.Variable(tf.zeros([self._output_neuron_count]), dtype=tf.float32)\n",
        "\n",
        "        self.deviation = None\n",
        "\n",
        "        self.optimizer = tf.optimizers.Adam()\n",
        "        self.checkpoint = tf.train.Checkpoint(model=self, optimizer=self.optimizer)\n",
        "        self.checkpoint_dir = home_dir + \"checkpoints/rnn_model/\"\n",
        "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.checkpoint_dir, max_to_keep=3)\n",
        "\n",
        "    def summary(self):\n",
        "        for i in range(len(self._hidden_w_list)):\n",
        "            print(f\"Recurrent hidden layer №{i+1}: {self._hidden_w_list[i].shape}\")\n",
        "            print(f\"    Previous hidden layer №{i+1}: {self._context_hidden_w_list[i].shape}\")\n",
        "        print(f\"Output layer: {self._output_w.shape}\")\n",
        "\n",
        "    def fit(self, train_data, train_answers, epochs=100, learning_rate=0.05, batch_size=16):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.deviation = max(train_answers) / 100\n",
        "        self.optimizer.learning_rate = learning_rate\n",
        "\n",
        "        mse_list = []\n",
        "        accuracy_list = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_mse = []\n",
        "            epoch_accuracy_numerator = 0\n",
        "\n",
        "            for i in range(0, len(train_data) - batch_size):\n",
        "                batch_data = train_data[i:i + batch_size]\n",
        "                batch_answers = train_answers[i:i + batch_size]\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    output = self._fit_passage(batch_data)\n",
        "                    mse = self._compute_mse(output, batch_answers)\n",
        "\n",
        "                trainable_vars = (\n",
        "                        self._hidden_w_list +\n",
        "                        self._hidden_b_list +\n",
        "                        self._context_hidden_w_list\n",
        "                        +\n",
        "                        [self._output_w, self._output_b]\n",
        "                )\n",
        "                gradients = tape.gradient(mse, trainable_vars)\n",
        "                self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "                epoch_mse.append(mse.numpy())\n",
        "\n",
        "            mean_mse = tf.reduce_mean(epoch_mse).numpy()\n",
        "            mse_list.append(mean_mse)\n",
        "            epoch_accuracy = epoch_accuracy_numerator / len(train_data)\n",
        "            accuracy_list.append(epoch_accuracy)\n",
        "\n",
        "            if (epoch + 1) % ((epochs + 1) // 10) == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}: MSE={mean_mse:.10f}, Accuracy={epoch_accuracy:.4f}\")\n",
        "\n",
        "        print(f\"Fit ended in {time.time() - start_time:.2f} secs.\")\n",
        "        if sum(accuracy_list[-3:]) / 3 > 0.9:\n",
        "            self.save_model()\n",
        "\n",
        "        statistic = {\n",
        "            'network': 'RNN',\n",
        "            'accuracy': accuracy_list,\n",
        "            'mse': mse_list,\n",
        "            'epochs': epochs,\n",
        "            'batch_size': batch_size,\n",
        "            'execution_time': time.time() - start_time,\n",
        "            'hidden_layer_count': self._hidden_layer_count,\n",
        "            'hidden_neurons_count': str(self._hidden_neurons_counts)\n",
        "        }\n",
        "        save_json(home_dir + '/statistics/rnn_statistic.txt', statistic)\n",
        "        self.fit_info(statistic)\n",
        "        return statistic\n",
        "\n",
        "    def _fit_passage(self, batch):\n",
        "        outputs = []\n",
        "        for _input in batch:\n",
        "            output = tf.transpose(_input)\n",
        "            for i in range(self._hidden_layer_count):\n",
        "                output = self._activation_tanh(\n",
        "                    tf.matmul(output, self._hidden_w_list[i]) +\n",
        "                    tf.matmul(output, self._context_hidden_w_list[i]) +\n",
        "                    self._hidden_b_list[i])\n",
        "            output = tf.matmul(output, self._output_w) + self._output_b\n",
        "            outputs.append(output)\n",
        "        return tf.stack(outputs)\n",
        "\n",
        "    def _default_passage(self, batch):\n",
        "        output = batch\n",
        "        for i in range(self._hidden_layer_count):\n",
        "            output = tf.Variable((self._activation_tanh(\n",
        "                tf.matmul(output, self._hidden_w_list[i]) +\n",
        "                tf.matmul(output, self._context_hidden_w_list[i]) +\n",
        "                self._hidden_b_list[i])))\n",
        "        return tf.matmul(output, self._output_w) + self._output_b\n",
        "\n",
        "    @staticmethod\n",
        "    def _activation_tanh(x):\n",
        "        return tf.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_mse(output, y):\n",
        "        return tf.reduce_mean(tf.square(output - y))\n",
        "\n",
        "    def save_model(self):\n",
        "        save_path = self.checkpoint.save(file_prefix=self.checkpoint_dir + 'rnn_model')\n",
        "        print(f\"\\033[35mModel saved : {save_path}\\033[0m\\n\")\n",
        "\n",
        "    def load_model(self):\n",
        "        self.checkpoint.restore(tf.train.latest_checkpoint(self.checkpoint_dir))\n",
        "        print(\"\\033[35mModel loaded!\\033[0m\\n\")\n",
        "\n",
        "    def evaluate(self, test_data, test_answers):\n",
        "        errors = []\n",
        "        for i in range(len(test_data)):\n",
        "            output = self._default_passage(test_data[i])\n",
        "            error = abs(output - test_answers[i])\n",
        "            if error < self.deviation:\n",
        "                errors.append(True)\n",
        "            else:\n",
        "                errors.append(False)\n",
        "        return errors.count(True) / len(errors) * 100\n",
        "\n",
        "    def predict(self, sequence):\n",
        "        return self._default_passage(sequence)\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_info(statistic):\n",
        "        one_fit_statistic(statistic)\n",
        "\n",
        "\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  train_data, train_answers, test_data, test_answers = prepare_data(\"bitcoin\")\n",
        "  rnn = RNN(24, [128, 128], 1, 2)\n",
        "  rnn.summary()\n",
        "  rnn.fit(train_data, train_answers)\n",
        "  rnn.evaluate(test_data, test_answers)\n",
        "  last_48_hour = coin_parsing('bitcoin', 2)\n",
        "  sequence = last_48_hour['price'][:24]\n",
        "\n",
        "  sequence_scaled = scaler.transform(np.array(sequence).reshape(-1, 1)).reshape(1, 24, 1)\n",
        "\n",
        "  prediction = rnn.predict(sequence_scaled)\n",
        "\n",
        "  predicted_price = scaler.inverse_transform(prediction)\n",
        "  print(f\"Predicted price: {predicted_price[0][0]}\")\n",
        "  print(f\"Actual price: {last_48_hour['price'][24]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPticW09nb3pcpgArAQs/WL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}