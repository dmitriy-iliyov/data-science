{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1t6UYupjLG_iEGqNwJBQ9tXPwZhe7nGcx",
      "authorship_tag": "ABX9TyPYxVSoNjnF9mV4dPYeyh9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmitriy-iliyov/data-science/blob/main/neural-network/lab_4/notebook/lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x1l5LEERt8sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402b9418-504c-4b29-f591-a0906c157d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "!cp kaggle.json /root/.kaggle/\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "import kagglehub\n",
        "dataset_dir = '/drive/MyDrive/data/tiny-imagenet'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "!kaggle datasets download -d akash2sharma/tiny-imagenet -p {dataset_dir}\n",
        "\n",
        "!unzip -q {dataset_dir}/tiny-imagenet.zip -d {dataset_dir}"
      ],
      "metadata": {
        "id": "V6kVX1u9zpUv",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf25f5d5-14c5-4073-cf11-cff3d47dd5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/akash2sharma/tiny-imagenet\n",
            "License(s): unknown\n",
            "Downloading tiny-imagenet.zip to /drive/MyDrive/data/tiny-imagenet\n",
            " 95% 451M/474M [00:03<00:00, 162MB/s]\n",
            "100% 474M/474M [00:03<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "def custom_image_generator(labels_map, images_dir, num_classes=200):\n",
        "    class_names = sorted(set(labels_map.values()))\n",
        "    class_to_index = {name: index for index, name in enumerate(class_names)}\n",
        "    labels_map = {key: class_to_index[value] for key, value in labels_map.items()}\n",
        "\n",
        "    image_paths = [os.path.join(images_dir, filename) for filename in labels_map.keys()]\n",
        "    labels = [labels_map[filename] for filename in labels_map.keys()]\n",
        "\n",
        "    def load_image(image_path, label):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, [224, 224])\n",
        "        label = tf.one_hot(label, num_classes)\n",
        "        return image, label\n",
        "    print(len(image_paths))\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def get_code_name_map(line):\n",
        "    splited_s = line.split('\\t')\n",
        "    splited_s[1] = splited_s[1].replace('\\n', '')\n",
        "    return splited_s[0], splited_s[1]\n",
        "\n",
        "def get_classes_code_name_map(names_path):\n",
        "    class_names = {}\n",
        "    with open(names_path) as file:\n",
        "      for line in file:\n",
        "        class_code, class_name = get_code_name_map(line)\n",
        "        class_names[class_code] = class_name\n",
        "    return class_names\n",
        "\n",
        "def get_class_codes(names_path):\n",
        "    class_codes = []\n",
        "    with open(names_path) as file:\n",
        "        for line in file:\n",
        "            class_codes.append(str(line).strip())\n",
        "    return class_codes\n",
        "\n",
        "def create_datasets():\n",
        "  home_dir = '/drive/MyDrive/data/tiny-imagenet/tiny-imagenet-200'\n",
        "  needed_class_path = home_dir + '/wnids.txt'\n",
        "  all_class_path = home_dir + '/words.txt'\n",
        "  train_dataset_path = home_dir + '/train'\n",
        "  test_dataset_path = home_dir + '/val/images'\n",
        "  width = 224\n",
        "  height = 224\n",
        "  batch_size = 32\n",
        "\n",
        "  train_dataset = tf.keras.utils.image_dataset_from_directory(train_dataset_path,\n",
        "                                                              image_size = (width, height),\n",
        "                                                              batch_size = batch_size,\n",
        "                                                              label_mode=\"categorical\",\n",
        "                                                              validation_split = .15,\n",
        "                                                              subset = 'training',\n",
        "                                                              seed = 341)\n",
        "\n",
        "  val_dataset = tf.keras.utils.image_dataset_from_directory (train_dataset_path,\n",
        "                                                             image_size = (width, height),\n",
        "                                                             batch_size = batch_size,\n",
        "                                                             label_mode=\"categorical\",\n",
        "                                                             validation_split = .15,\n",
        "                                                             subset = 'validation',\n",
        "                                                             seed = 341)\n",
        "\n",
        "  needed_class_codes = get_class_codes(needed_class_path)\n",
        "  all_class_code_name_map = get_classes_code_name_map(all_class_path)\n",
        "  class_code_name_map = {}\n",
        "  classes_indexes = []\n",
        "  for index, code in enumerate(needed_class_codes):\n",
        "      class_code_name_map[code] = all_class_code_name_map[code]\n",
        "      classes_indexes.append(index)\n",
        "  train_lables = keras.utils.to_categorical(classes_indexes, 200)\n",
        "\n",
        "  train_dataset.class_names = train_lables\n",
        "  val_dataset.class_names = train_lables\n",
        "\n",
        "  test_dataset_path_names = home_dir + '/val/val_annotations.txt'\n",
        "  class_imgname_code_map = get_classes_code_name_map(test_dataset_path_names)\n",
        "  test_dataset = custom_image_generator(class_imgname_code_map, test_dataset_path)\n",
        "\n",
        "  print(train_dataset.class_names)\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "EVJRsTDna2io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import Sequential, Input\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, ZeroPadding2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "class AlexNet:\n",
        "    def __init__(self):\n",
        "        self.model = Sequential([\n",
        "            Input((224, 224, 3)),\n",
        "            Conv2D(96, (11, 11), strides=4, padding='valid', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            ZeroPadding2D(padding=(2, 2)),\n",
        "            Conv2D(256, (5, 5), strides=1, padding='valid', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "            Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "            Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            Flatten(),\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(200, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        self.store_path = '/content/drive/MyDrive/'\n",
        "        self.class_names = []\n",
        "\n",
        "        print(self.model.summary())\n",
        "\n",
        "        self.model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                           loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def fit(self, train_dataset, val_dataset, epochs=20):\n",
        "        self.class_names = train_dataset.class_names\n",
        "        start = time.time()\n",
        "        history = self.model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
        "        execution_time = time.time() - start\n",
        "        self.model.save(self.store_path + '/lab_4_model.keras')\n",
        "        self.plot_history(history, epochs, execution_time)\n",
        "\n",
        "\n",
        "    def evaluete(self, test_dataset):\n",
        "      loss, accuracy = self.model.evaluate(test_dataset)\n",
        "      print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "    def\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_history(history, epochs, execution_time):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(1, epochs + 1), history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(range(1, epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Accuracy (Execution Time: {execution_time:.2f} seconds)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(1, epochs + 1), history.history['loss'], label='Training Loss')\n",
        "        plt.plot(range(1, epochs + 1), history.history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "cp4q7nE1oS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = create_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpFGOOBA9gJv",
        "outputId": "7f8873fe-c71f-465b-8075-02d40c0bf09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100000 files belonging to 200 classes.\n",
            "Using 85000 files for training.\n",
            "Found 100000 files belonging to 200 classes.\n",
            "Using 15000 files for validation.\n"]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alex_net_model = AlexNet()\n",
        "with tf.device('/GPU:0'):\n",
        "    alex_net_model.fit(train_dataset, val_dataset, 20)"
      ],
      "metadata": {
        "id": "Ee_m7Pi3p3Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alex_net_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "f-XxB_CjqCak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
