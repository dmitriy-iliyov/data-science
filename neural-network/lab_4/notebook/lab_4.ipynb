{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1t6UYupjLG_iEGqNwJBQ9tXPwZhe7nGcx",
      "authorship_tag": "ABX9TyPYxVSoNjnF9mV4dPYeyh9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmitriy-iliyov/data-science/blob/main/neural-network/lab_4/notebook/lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x1l5LEERt8sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402b9418-504c-4b29-f591-a0906c157d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "!cp kaggle.json /root/.kaggle/\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "import kagglehub\n",
        "dataset_dir = '/drive/MyDrive/data/tiny-imagenet'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "!kaggle datasets download -d akash2sharma/tiny-imagenet -p {dataset_dir}\n",
        "\n",
        "!unzip -q {dataset_dir}/tiny-imagenet.zip -d {dataset_dir}"
      ],
      "metadata": {
        "id": "V6kVX1u9zpUv",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf25f5d5-14c5-4073-cf11-cff3d47dd5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/akash2sharma/tiny-imagenet\n",
            "License(s): unknown\n",
            "Downloading tiny-imagenet.zip to /drive/MyDrive/data/tiny-imagenet\n",
            " 95% 451M/474M [00:03<00:00, 162MB/s]\n",
            "100% 474M/474M [00:03<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "def custom_image_generator(labels_map, images_dir, num_classes=200):\n",
        "    class_names = sorted(set(labels_map.values()))\n",
        "    class_to_index = {name: index for index, name in enumerate(class_names)}\n",
        "    labels_map = {key: class_to_index[value] for key, value in labels_map.items()}\n",
        "\n",
        "    image_paths = [os.path.join(images_dir, filename) for filename in labels_map.keys()]\n",
        "    labels = [labels_map[filename] for filename in labels_map.keys()]\n",
        "\n",
        "    def load_image(image_path, label):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, [224, 224])\n",
        "        label = tf.one_hot(label, num_classes)\n",
        "        return image, label\n",
        "    print(len(image_paths))\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def get_code_name_map(line):\n",
        "    splited_s = line.split('\\t')\n",
        "    splited_s[1] = splited_s[1].replace('\\n', '')\n",
        "    return splited_s[0], splited_s[1]\n",
        "\n",
        "def get_classes_code_name_map(names_path):\n",
        "    class_names = {}\n",
        "    with open(names_path) as file:\n",
        "      for line in file:\n",
        "        class_code, class_name = get_code_name_map(line)\n",
        "        class_names[class_code] = class_name\n",
        "    return class_names\n",
        "\n",
        "def get_class_codes(names_path):\n",
        "    class_codes = []\n",
        "    with open(names_path) as file:\n",
        "        for line in file:\n",
        "            class_codes.append(str(line).strip())\n",
        "    return class_codes\n",
        "\n",
        "def create_datasets():\n",
        "  home_dir = '/drive/MyDrive/data/tiny-imagenet/tiny-imagenet-200'\n",
        "  needed_class_path = home_dir + '/wnids.txt'\n",
        "  all_class_path = home_dir + '/words.txt'\n",
        "  train_dataset_path = home_dir + '/train'\n",
        "  test_dataset_path = home_dir + '/val/images'\n",
        "  width = 224\n",
        "  height = 224\n",
        "  batch_size = 32\n",
        "\n",
        "  train_dataset = tf.keras.utils.image_dataset_from_directory(train_dataset_path,\n",
        "                                                              image_size = (width, height),\n",
        "                                                              batch_size = batch_size,\n",
        "                                                              label_mode=\"categorical\",\n",
        "                                                              validation_split = .15,\n",
        "                                                              subset = 'training',\n",
        "                                                              seed = 341)\n",
        "\n",
        "  val_dataset = tf.keras.utils.image_dataset_from_directory (train_dataset_path,\n",
        "                                                             image_size = (width, height),\n",
        "                                                             batch_size = batch_size,\n",
        "                                                             label_mode=\"categorical\",\n",
        "                                                             validation_split = .15,\n",
        "                                                             subset = 'validation',\n",
        "                                                             seed = 341)\n",
        "\n",
        "  needed_class_codes = get_class_codes(needed_class_path)\n",
        "  all_class_code_name_map = get_classes_code_name_map(all_class_path)\n",
        "  class_code_name_map = {}\n",
        "  classes_indexes = []\n",
        "  for index, code in enumerate(needed_class_codes):\n",
        "      class_code_name_map[code] = all_class_code_name_map[code]\n",
        "      classes_indexes.append(index)\n",
        "  train_lables = keras.utils.to_categorical(classes_indexes, 200)\n",
        "\n",
        "  train_dataset.class_names = train_lables\n",
        "  val_dataset.class_names = train_lables\n",
        "\n",
        "  test_dataset_path_names = home_dir + '/val/val_annotations.txt'\n",
        "  class_imgname_code_map = get_classes_code_name_map(test_dataset_path_names)\n",
        "  test_dataset = custom_image_generator(class_imgname_code_map, test_dataset_path)\n",
        "\n",
        "  print(train_dataset.class_names)\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "EVJRsTDna2io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import Sequential, Input\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, ZeroPadding2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "class AlexNet:\n",
        "    def __init__(self):\n",
        "        self.model = Sequential([\n",
        "            Input((224, 224, 3)),\n",
        "            Conv2D(96, (11, 11), strides=4, padding='valid', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            ZeroPadding2D(padding=(2, 2)),\n",
        "            Conv2D(256, (5, 5), strides=1, padding='valid', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "            Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "            Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
        "            Flatten(),\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(200, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        self.store_path = '/content/drive/MyDrive/'\n",
        "        self.class_names = []\n",
        "\n",
        "        print(self.model.summary())\n",
        "\n",
        "        self.model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                           loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def fit(self, train_dataset, val_dataset, epochs=20):\n",
        "        self.class_names = train_dataset.class_names\n",
        "        start = time.time()\n",
        "        history = self.model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
        "        execution_time = time.time() - start\n",
        "        self.model.save(self.store_path + '/lab_4_model.keras')\n",
        "        self.plot_history(history, epochs, execution_time)\n",
        "\n",
        "\n",
        "    def evaluete(self, test_dataset):\n",
        "      loss, accuracy = self.model.evaluate(test_dataset)\n",
        "      print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "    def\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_history(history, epochs, execution_time):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(1, epochs + 1), history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(range(1, epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Accuracy (Execution Time: {execution_time:.2f} seconds)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(1, epochs + 1), history.history['loss'], label='Training Loss')\n",
        "        plt.plot(range(1, epochs + 1), history.history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "cp4q7nE1oS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = create_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpFGOOBA9gJv",
        "outputId": "7f8873fe-c71f-465b-8075-02d40c0bf09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100000 files belonging to 200 classes.\n",
            "Using 85000 files for training.\n",
            "Found 100000 files belonging to 200 classes.\n",
            "Using 15000 files for validation.\n",
            "{'n02124075': 'Egyptian cat', 'n04067472': 'reel', 'n04540053': 'volleyball', 'n04099969': 'rocking chair, rocker', 'n07749582': 'lemon', 'n01641577': 'bullfrog, Rana catesbeiana', 'n02802426': 'basketball', 'n09246464': 'cliff, drop, drop-off', 'n07920052': 'espresso', 'n03970156': \"plunger, plumber's helper\", 'n03891332': 'parking meter', 'n02106662': 'German shepherd, German shepherd dog, German police dog, alsatian', 'n03201208': 'dining table, board', 'n02279972': 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'n02132136': 'brown bear, bruin, Ursus arctos', 'n04146614': 'school bus', 'n07873807': 'pizza, pizza pie', 'n02364673': 'guinea pig, Cavia cobaya', 'n04507155': 'umbrella', 'n03854065': 'organ, pipe organ', 'n03838899': 'oboe, hautboy, hautbois', 'n03733131': 'maypole', 'n01443537': 'goldfish, Carassius auratus', 'n07875152': 'potpie', 'n03544143': 'hourglass', 'n09428293': 'seashore, coast, seacoast, sea-coast', 'n03085013': 'computer keyboard, keypad', 'n02437312': 'Arabian camel, dromedary, Camelus dromedarius', 'n07614500': 'ice cream, icecream', 'n03804744': 'nail', 'n04265275': 'space heater', 'n02963159': 'cardigan', 'n02486410': 'baboon', 'n01944390': 'snail', 'n09256479': 'coral reef', 'n02058221': 'albatross, mollymawk', 'n04275548': \"spider web, spider's web\", 'n02321529': 'sea cucumber, holothurian', 'n02769748': 'backpack, back pack, knapsack, packsack, rucksack, haversack', 'n02099712': 'Labrador retriever', 'n07695742': 'pretzel', 'n02056570': 'king penguin, Aptenodytes patagonica', 'n02281406': 'sulphur butterfly, sulfur butterfly', 'n01774750': 'tarantula', 'n02509815': 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 'n03983396': 'pop bottle, soda bottle', 'n07753592': 'banana', 'n04254777': 'sock', 'n02233338': 'cockroach, roach', 'n04008634': 'projectile, missile', 'n02823428': 'beer bottle', 'n02236044': 'mantis, mantid', 'n03393912': 'freight car', 'n07583066': 'guacamole', 'n04074963': 'remote control, remote', 'n01629819': 'European fire salamander, Salamandra salamandra', 'n09332890': 'lakeside, lakeshore', 'n02481823': 'chimpanzee, chimp, Pan troglodytes', 'n03902125': 'pay-phone, pay-station', 'n03404251': 'fur coat', 'n09193705': 'alp', 'n03637318': 'lampshade, lamp shade', 'n04456115': 'torch', 'n02666196': 'abacus', 'n03796401': 'moving van', 'n02795169': 'barrel, cask', 'n02123045': 'tabby, tabby cat', 'n01855672': 'goose', 'n01882714': 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'n02917067': 'bullet train, bullet', 'n02988304': 'CD player', 'n04398044': 'teapot', 'n02843684': 'birdhouse', 'n02423022': 'gazelle', 'n02669723': \"academic gown, academic robe, judge's robe\", 'n04465501': 'tractor', 'n02165456': 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 'n03770439': 'miniskirt, mini', 'n02099601': 'golden retriever', 'n04486054': 'triumphal arch', 'n02950826': 'cannon', 'n03814639': 'neck brace', 'n04259630': 'sombrero', 'n03424325': 'gasmask, respirator, gas helmet', 'n02948072': 'candle, taper, wax light', 'n03179701': 'desk', 'n03400231': 'frying pan, frypan, skillet', 'n02206856': 'bee', 'n03160309': 'dam, dike, dyke', 'n01984695': 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', 'n03977966': 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 'n03584254': 'iPod', 'n04023962': 'punching bag, punch bag, punching ball, punchball', 'n02814860': 'beacon, lighthouse, beacon light, pharos', 'n01910747': 'jellyfish', 'n04596742': 'wok', 'n03992509': \"potter's wheel\", 'n04133789': 'sandal', 'n03937543': 'pill bottle', 'n02927161': 'butcher shop, meat market', 'n01945685': 'slug', 'n02395406': 'hog, pig, grunter, squealer, Sus scrofa', 'n02125311': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'n03126707': 'crane', 'n04532106': 'vestment', 'n02268443': \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 'n02977058': 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'n07734744': 'mushroom', 'n03599486': 'jinrikisha, ricksha, rickshaw', 'n04562935': 'water tower', 'n03014705': 'chest', 'n04251144': 'snorkel', 'n04356056': 'sunglasses, dark glasses, shades', 'n02190166': 'fly', 'n03670208': 'limousine, limo', 'n02002724': 'black stork, Ciconia nigra', 'n02074367': 'dugong, Dugong dugon', 'n04285008': 'sports car, sport car', 'n04560804': 'water jug', 'n04366367': 'suspension bridge', 'n02403003': 'ox', 'n07615774': 'ice lolly, lolly, lollipop, popsicle', 'n04501370': 'turnstile', 'n03026506': 'Christmas stocking', 'n02906734': 'broom', 'n01770393': 'scorpion', 'n04597913': 'wooden spoon', 'n03930313': 'picket fence, paling', 'n04118538': 'rugby ball', 'n04179913': 'sewing machine', 'n04311004': 'steel arch bridge', 'n02123394': 'Persian cat', 'n04070727': 'refrigerator, icebox', 'n02793495': 'barn', 'n02730930': 'apron', 'n02094433': 'Yorkshire terrier', 'n04371430': 'swimming trunks, bathing trunks', 'n04328186': 'stopwatch, stop watch', 'n03649909': 'lawn mower, mower', 'n04417672': 'thatch, thatched roof', 'n03388043': 'fountain', 'n01774384': 'black widow, Latrodectus mactans', 'n02837789': 'bikini, two-piece', 'n07579787': 'plate', 'n04399382': 'teddy, teddy bear', 'n02791270': 'barbershop', 'n03089624': 'confectionery, confectionary, candy store', 'n02814533': 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 'n04149813': 'scoreboard', 'n07747607': 'orange', 'n03355925': 'flagpole, flagstaff', 'n01983481': 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 'n04487081': 'trolleybus, trolley coach, trackless trolley', 'n03250847': 'drumstick', 'n03255030': 'dumbbell', 'n02892201': 'brass, memorial tablet, plaque', 'n02883205': 'bow tie, bow-tie, bowtie', 'n03100240': 'convertible', 'n02415577': 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 'n02480495': 'orangutan, orang, orangutang, Pongo pygmaeus', 'n01698640': 'American alligator, Alligator mississipiensis', 'n01784675': 'centipede', 'n04376876': 'syringe', 'n03444034': 'go-kart', 'n01917289': 'brain coral', 'n01950731': 'sea slug, nudibranch', 'n03042490': 'cliff dwelling', 'n07711569': 'mashed potato', 'n04532670': 'viaduct', 'n03763968': 'military uniform', 'n07768694': 'pomegranate', 'n02999410': 'chain', 'n03617480': 'kimono', 'n06596364': 'comic book', 'n01768244': 'trilobite', 'n02410509': 'bison', 'n03976657': 'pole', 'n01742172': 'boa constrictor, Constrictor constrictor', 'n03980874': 'poncho', 'n02808440': 'bathtub, bathing tub, bath, tub', 'n02226429': 'grasshopper, hopper', 'n02231487': 'walking stick, walkingstick, stick insect', 'n02085620': 'Chihuahua', 'n01644900': 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 'n02129165': 'lion, king of beasts, Panthera leo', 'n02699494': 'altar', 'n03837869': 'obelisk', 'n02815834': 'beaker', 'n07720875': 'bell pepper', 'n02788148': 'bannister, banister, balustrade, balusters, handrail', 'n02909870': 'bucket, pail', 'n03706229': 'magnetic compass', 'n07871810': 'meat loaf, meatloaf', 'n03447447': 'gondola', 'n02113799': 'standard poodle', 'n12267677': 'acorn', 'n03662601': 'lifeboat', 'n02841315': 'binoculars, field glasses, opera glasses', 'n07715103': 'cauliflower', 'n02504458': 'African elephant, Loxodonta africana'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alex_net_model = AlexNet()\n",
        "with tf.device('/GPU:0'):\n",
        "    alex_net_model.fit(train_dataset, val_dataset, 20)"
      ],
      "metadata": {
        "id": "Ee_m7Pi3p3Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alex_net_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "f-XxB_CjqCak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}